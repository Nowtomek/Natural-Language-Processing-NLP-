{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Twitter US Airline Sentiment Classifier \n",
    "Twitter US Airline Sentiment\n",
    "\n",
    "A sentiment analysis job about the problems of each major U.S. airline.\n",
    "Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\n",
    "The project is from a dataset from Kaggle.\n",
    "Link to the Kaggle project site:https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "The dataset has to be downloaded from the above Kagglewebsite.\n",
    "In [1]:\n",
    "# install and import necessary libraries.\n",
    "\n",
    "# !pip install contractions\n",
    "\n",
    "import re, string, unicodedata                          # Import Regex, string and unicodedata.\n",
    "import contractions                                     # Import contractions library.\n",
    "from bs4 import BeautifulSoup                           # Import BeautifulSoup.\n",
    "\n",
    "import numpy as np                                      # Import numpy.\n",
    "import pandas as pd                                     # Import pandas.\n",
    "import nltk                                             # Import Natural Language Tool-Kit.\n",
    "\n",
    "nltk.download('stopwords')                              # Download Stopwords.\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords                       # Import stopwords.\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer. For removing stem words\n",
    "import unicodedata                                      # Removing accented characters\n",
    "[nltk_data] Downloading package stopwords to\n",
    "[nltk_data]     C:\\Users\\91905\\AppData\\Roaming\\nltk_data...\n",
    "[nltk_data]   Package stopwords is already up-to-date!\n",
    "[nltk_data] Downloading package punkt to\n",
    "[nltk_data]     C:\\Users\\91905\\AppData\\Roaming\\nltk_data...\n",
    "[nltk_data]   Package punkt is already up-to-date!\n",
    "[nltk_data] Downloading package wordnet to\n",
    "[nltk_data]     C:\\Users\\91905\\AppData\\Roaming\\nltk_data...\n",
    "[nltk_data]   Package wordnet is already up-to-date!\n",
    "Reading the Data \n",
    "In [2]:\n",
    "# Loading data into pandas dataframe\n",
    "data = pd.read_csv(\"Tweets.csv\")\n",
    "In [3]:\n",
    "data.shape                                               # print shape of data.\n",
    "Out[3]:\n",
    "(14640, 15)\n",
    "In [4]:\n",
    "data.head(2)                                              # Print first 5 rows of data.\n",
    "Out[4]:\n",
    "tweet_id\tairline_sentiment\tairline_sentiment_confidence\tnegativereason\tnegativereason_confidence\tairline\tairline_sentiment_gold\tname\tnegativereason_gold\tretweet_count\ttext\ttweet_coord\ttweet_created\ttweet_location\tuser_timezone\n",
    "0\t570306133677760513\tneutral\t1.0000\tNaN\tNaN\tVirgin America\tNaN\tcairdin\tNaN\t0\t@VirginAmerica What @dhepburn said.\tNaN\t2015-02-24 11:35:52 -0800\tNaN\tEastern Time (US & Canada)\n",
    "1\t570301130888122368\tpositive\t0.3486\tNaN\t0.0\tVirgin America\tNaN\tjnardino\tNaN\t0\t@VirginAmerica plus you've added commercials t...\tNaN\t2015-02-24 11:15:59 -0800\tNaN\tPacific Time (US & Canada)\n",
    "In [5]:\n",
    "data.isnull().sum(axis=0)                                # Check for NULL values.\n",
    "Out[5]:\n",
    "tweet_id                            0\n",
    "airline_sentiment                   0\n",
    "airline_sentiment_confidence        0\n",
    "negativereason                   5462\n",
    "negativereason_confidence        4118\n",
    "airline                             0\n",
    "airline_sentiment_gold          14600\n",
    "name                                0\n",
    "negativereason_gold             14608\n",
    "retweet_count                       0\n",
    "text                                0\n",
    "tweet_coord                     13621\n",
    "tweet_created                       0\n",
    "tweet_location                   4733\n",
    "user_timezone                    4820\n",
    "dtype: int64\n",
    "Building the dataset for further Deep Dive \n",
    "For text classification and model development, columns text and airline_sentiment are chosen.\n",
    "\n",
    "In [6]:\n",
    "data = data[['text', 'airline_sentiment']]\n",
    "In [7]:\n",
    "pd.set_option('display.max_colwidth', None) # Display full dataframe information (Non-turncated Text column.)\n",
    "\n",
    "data.head()                                 # Check first 5 rows of data\n",
    "Out[7]:\n",
    "text\tairline_sentiment\n",
    "0\t@VirginAmerica What @dhepburn said.\tneutral\n",
    "1\t@VirginAmerica plus you've added commercials to the experience... tacky.\tpositive\n",
    "2\t@VirginAmerica I didn't today... Must mean I need to take another trip!\tneutral\n",
    "3\t@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\tnegative\n",
    "4\t@VirginAmerica and it's a really big bad thing about it\tnegative\n",
    "In [8]:\n",
    "data['airline_sentiment'].unique()\n",
    "Out[8]:\n",
    "array(['neutral', 'positive', 'negative'], dtype=object)\n",
    "In [9]:\n",
    "data['airline_sentiment'].value_counts()\n",
    "Out[9]:\n",
    "negative    9178\n",
    "neutral     3099\n",
    "positive    2363\n",
    "Name: airline_sentiment, dtype: int64\n",
    "Data Preprocessing \n",
    "Remove html tags using BeautifulSoup\n",
    "Remove https using regex\n",
    "Replace contractions in string. (e.g. replace I'm --> I am) and so on using contraction.fix()\n",
    "Remove numbers using re(), re is basicially regex function\n",
    "Check Language of the Tweet\n",
    "Tokenization - Converting lines into token or words\n",
    "To remove Stopword - removing unnecessary words, note that since this is a sentiment analysis only a few stopwords are used\n",
    "Lemmatized data - lematization\n",
    "We have used NLTK library to tokenize words , remove stopwords and lemmatize the remaining words\n",
    "HTML Tag Removal\n",
    "In [10]:\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: strip_html(x))\n",
    "data.head()\n",
    "Out[10]:\n",
    "text\tairline_sentiment\n",
    "0\t@VirginAmerica What @dhepburn said.\tneutral\n",
    "1\t@VirginAmerica plus you've added commercials to the experience... tacky.\tpositive\n",
    "2\t@VirginAmerica I didn't today... Must mean I need to take another trip!\tneutral\n",
    "3\t@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse\tnegative\n",
    "4\t@VirginAmerica and it's a really big bad thing about it\tnegative\n",
    "In [11]:\n",
    "for i, row in data.iterrows():\n",
    "    clean_text = re.sub(r\"http\\S+\", \"\", data.at[i, 'text']) #--Extracting anything that follows http till a space is encountered\n",
    "    clean_text = re.sub(r\"https\",'',clean_text)\n",
    "    data.at[i,'text'] = clean_text\n",
    "data.head()\n",
    "Out[11]:\n",
    "text\tairline_sentiment\n",
    "0\t@VirginAmerica What @dhepburn said.\tneutral\n",
    "1\t@VirginAmerica plus you've added commercials to the experience... tacky.\tpositive\n",
    "2\t@VirginAmerica I didn't today... Must mean I need to take another trip!\tneutral\n",
    "3\t@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse\tnegative\n",
    "4\t@VirginAmerica and it's a really big bad thing about it\tnegative\n",
    "Conraction Removal\n",
    "In [12]:\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: replace_contractions(x))\n",
    "data.head()\n",
    "Out[12]:\n",
    "text\tairline_sentiment\n",
    "0\t@VirginAmerica What @dhepburn said.\tneutral\n",
    "1\t@VirginAmerica plus you have added commercials to the experience... tacky.\tpositive\n",
    "2\t@VirginAmerica I did not today... Must mean I need to take another trip!\tneutral\n",
    "3\t@VirginAmerica it is really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse\tnegative\n",
    "4\t@VirginAmerica and it is a really big bad thing about it\tnegative\n",
    "Numbers Removal\n",
    "In [13]:\n",
    "def remove_numbers(text):\n",
    "  text = re.sub(r'\\d+', '', text)\n",
    "  return text\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: remove_numbers(x))\n",
    "data.head()\n",
    "Out[13]:\n",
    "text\tairline_sentiment\n",
    "0\t@VirginAmerica What @dhepburn said.\tneutral\n",
    "1\t@VirginAmerica plus you have added commercials to the experience... tacky.\tpositive\n",
    "2\t@VirginAmerica I did not today... Must mean I need to take another trip!\tneutral\n",
    "3\t@VirginAmerica it is really aggressive to blast obnoxious \"entertainment\" in your guests' faces & they have little recourse\tnegative\n",
    "4\t@VirginAmerica and it is a really big bad thing about it\tnegative\n",
    "Language Check \n",
    "In [ ]:\n",
    "from langdetect import detect\n",
    "\n",
    "data['Language'] = \"\"\n",
    "\n",
    "for i, desc in enumerate(data['text']):\n",
    "    try:\n",
    "        data['Language'][i] = detect(desc)\n",
    "    except:\n",
    "        data['Language'][i] = 'unknown'\n",
    "In [ ]:\n",
    "data['Language'].value_counts()\n",
    "In [ ]:\n",
    "data[data['Language']=='it'].head()\n",
    "Tokenizer \n",
    "In [14]:\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr=TweetTokenizer()\n",
    "In [15]:\n",
    "data['text'] = data.apply(lambda row: tknzr.tokenize(row['text']), axis=1) # Tokenization of data\n",
    "In [16]:\n",
    "data.head()                                                                    # Look at how tokenized data looks.\n",
    "Out[16]:\n",
    "text\tairline_sentiment\n",
    "0\t[@VirginAmerica, What, @dhepburn, said, .]\tneutral\n",
    "1\t[@VirginAmerica, plus, you, have, added, commercials, to, the, experience, ..., tacky, .]\tpositive\n",
    "2\t[@VirginAmerica, I, did, not, today, ..., Must, mean, I, need, to, take, another, trip, !]\tneutral\n",
    "3\t[@VirginAmerica, it, is, really, aggressive, to, blast, obnoxious, \", entertainment, \", in, your, guests, ', faces, &, they, have, little, recourse]\tnegative\n",
    "4\t[@VirginAmerica, and, it, is, a, really, big, bad, thing, about, it]\tnegative\n",
    "Stopwords \n",
    "In [17]:\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "customlist = ['not', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
    "        \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
    "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "        \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "# Set custom stop-word's list as not, couldn't etc. words matter in Sentiment, so not removing them from original data.\n",
    "\n",
    "stopwords = list(set(stopwords) - set(customlist))\n",
    "Normalize \n",
    "In [19]:\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_list(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "      new_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize_list(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "data['text'] = data.apply(lambda row: normalize(row['text']), axis=1)\n",
    "data.head()\n",
    "Out[19]:\n",
    "text\tairline_sentiment\n",
    "0\tvirginamerica dhepburn say\tneutral\n",
    "1\tvirginamerica plus add commercials experience tacky\tpositive\n",
    "2\tvirginamerica not today must mean need take another trip\tneutral\n",
    "3\tvirginamerica really aggressive blast obnoxious entertainment guests face little recourse\tnegative\n",
    "4\tvirginamerica really big bad thing\tnegative\n",
    "Count Vectorizer \n",
    "In [20]:\n",
    "# Vectorization (Convert text data to numbers).\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)                # Keep only 5000 features as number of features will increase the processing time.\n",
    "data_features = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "data_features = data_features.toarray()                        # Convert the data features to array.\n",
    "In [21]:\n",
    "data_features.shape\n",
    "Out[21]:\n",
    "(14640, 10000)\n",
    "In [22]:\n",
    "data_features[0].sum()\n",
    "Out[22]:\n",
    "3\n",
    "In [23]:\n",
    "X = data_features\n",
    "\n",
    "y = data.airline_sentiment\n",
    "Stratified K Fold - Cross Validation \n",
    "In [24]:\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "In [25]:\n",
    "# Split data into training and testing set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "Model Development using Count Vectorize \n",
    "In [26]:\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "In [27]:\n",
    "model_nb = MultinomialNB()\n",
    "model_dt = DecisionTreeClassifier(criterion='entropy', random_state=22)\n",
    "model_rf = RandomForestClassifier(max_samples=0.8, random_state=22)\n",
    "model_bg = BaggingClassifier(bootstrap=True, random_state=22)\n",
    "model_ab = AdaBoostClassifier(random_state=22)\n",
    "model_gb=GradientBoostingClassifier(random_state=22)\n",
    "In [28]:\n",
    "models=[model_nb,model_dt, model_rf, model_bg]\n",
    "model_nm=['Naive Bayes','Decision Tree','Random Forest','Bagging']\n",
    "In [29]:\n",
    "model_name=[]\n",
    "technique_type=[]\n",
    "train_score=[]\n",
    "test_score=[]\n",
    "cv_min=[]\n",
    "cv_max=[]\n",
    "cv_av=[]\n",
    "cv_std=[]\n",
    "std_score=[]\n",
    "\n",
    "i=0\n",
    "for m in models:\n",
    "    \n",
    "    m.fit(X_train, y_train)\n",
    "    \n",
    "    print(model_nm[i])\n",
    "    \n",
    "    model_name.append(model_nm[i])\n",
    "    i+=1\n",
    "    \n",
    "    train_score.append(m.score(X_train,y_train))\n",
    "    test_score.append(m.score(X_test,y_test))\n",
    "    \n",
    "    sf=StratifiedKFold(n_splits=5, random_state=20, shuffle=True)\n",
    "    kf=cross_val_score(m, X_train, y_train, cv=sf)\n",
    "    \n",
    "    cv_min.append(np.min(kf))\n",
    "    cv_max.append(np.max(kf))\n",
    "    cv_av.append(np.mean(kf))\n",
    "    cv_std.append(np.std(kf))\n",
    "    technique_type.append('Count Vectorize')\n",
    "Naive Bayes\n",
    "Decision Tree\n",
    "Random Forest\n",
    "Bagging\n",
    "In [30]:\n",
    "test_score\n",
    "Out[30]:\n",
    "[0.773224043715847, 0.7049180327868853, 0.7786885245901639, 0.7531876138433515]\n",
    "In [31]:\n",
    "kf\n",
    "Out[31]:\n",
    "array([0.72829268, 0.73317073, 0.72243902, 0.72620791, 0.72718399])\n",
    "Model Output using Count Vectorize \n",
    "In [32]:\n",
    "Results_Cv=pd.DataFrame()\n",
    "Results_Cv['Model']=model_name\n",
    "Results_Cv['Technique']=technique_type\n",
    "Results_Cv['Train Acc']=train_score\n",
    "Results_Cv['Cross Val - Min Accuarcy']=cv_min\n",
    "Results_Cv['Cross Val - Avg Accuarcy']=cv_av\n",
    "Results_Cv['Cross Val - Max Accuarcy']=cv_max\n",
    "Results_Cv['Cross Val - Std of Accuarcy']=cv_std\n",
    "Results_Cv['Test']=test_score\n",
    "Results_Cv\n",
    "Out[32]:\n",
    "Model\tTechnique\tTrain Acc\tCross Val - Min Accuarcy\tCross Val - Avg Accuarcy\tCross Val - Max Accuarcy\tCross Val - Std of Accuarcy\tTest\n",
    "0\tNaive Bayes\tCount Vectorize\t0.838115\t0.744265\t0.750097\t0.768293\t0.009181\t0.773224\n",
    "1\tDecision Tree\tCount Vectorize\t0.995219\t0.681796\t0.696135\t0.705710\t0.008318\t0.704918\n",
    "2\tRandom Forest\tCount Vectorize\t0.994340\t0.752074\t0.761417\t0.773659\t0.008311\t0.778689\n",
    "3\tBagging\tCount Vectorize\t0.971604\t0.722439\t0.727459\t0.733171\t0.003469\t0.753188\n",
    "Confusion Matrix \n",
    "In [33]:\n",
    "# Print and plot Confusion matirx to get an idea of how the distribution of the prediction is, among all the classes.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "result=model_rf.predict(X_test)\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, result)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "df_cm = pd.DataFrame(conf_mat, index = [i for i in ['negative', 'neutral','positive']],\n",
    "                  columns = [i for i in ['negative', 'neutral','positive']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')\n",
    "[[2595  169   50]\n",
    " [ 392  423   69]\n",
    " [ 192  100  402]]\n",
    "Out[33]:\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x1c323eafc48>\n",
    "\n",
    "TFIDF ID \n",
    "In [34]:\n",
    "# Using TfidfVectorizer to convert text data to numbers.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "data_features = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "data_features = data_features.toarray()\n",
    "\n",
    "data_features.shape\n",
    "Out[34]:\n",
    "(14640, 10000)\n",
    "In [35]:\n",
    "data_features[0].sum()\n",
    "Out[35]:\n",
    "1.5979352257716268\n",
    "In [36]:\n",
    "X = data_features\n",
    "\n",
    "y = data.airline_sentiment\n",
    "In [37]:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "In [38]:\n",
    "model_nb = MultinomialNB()\n",
    "model_dt = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "model_rf = RandomForestClassifier(max_samples=0.8, random_state=42)\n",
    "model_bg = BaggingClassifier(bootstrap=True, random_state=22)\n",
    "model_ab = AdaBoostClassifier(random_state=22)\n",
    "model_gb=GradientBoostingClassifier(random_state=22)\n",
    "In [39]:\n",
    "models=[model_nb,model_dt, model_rf, model_bg]\n",
    "model_nm=['Naive Bayes','Decision Tree','Random Forest','Bagging']\n",
    "In [40]:\n",
    "model_name=[]\n",
    "train_score=[]\n",
    "test_score=[]\n",
    "cv_min=[]\n",
    "cv_max=[]\n",
    "cv_av=[]\n",
    "cv_std=[]\n",
    "std_score=[]\n",
    "\n",
    "i=0\n",
    "for m in models:\n",
    "    \n",
    "    m.fit(X_train, y_train)\n",
    "    \n",
    "    print(model_nm[i])\n",
    "    \n",
    "    model_name.append(model_nm[i])\n",
    "    i+=1\n",
    "    \n",
    "    train_score.append(m.score(X_train,y_train))\n",
    "    test_score.append(m.score(X_test,y_test))\n",
    "    \n",
    "    sf=StratifiedKFold(n_splits=5, random_state=20, shuffle=True)\n",
    "    kf=cross_val_score(m, X_train, y_train, cv=sf)\n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_min.append(np.min(kf))\n",
    "    cv_max.append(np.max(kf))\n",
    "    cv_av.append(np.mean(kf))\n",
    "    cv_std.append(np.std(kf))\n",
    "Naive Bayes\n",
    "Decision Tree\n",
    "Random Forest\n",
    "Bagging\n",
    "In [41]:\n",
    "cv_min\n",
    "Out[41]:\n",
    "[0.6603221083455344,\n",
    " 0.6661786237188873,\n",
    " 0.7419512195121951,\n",
    " 0.7126829268292683]\n",
    "In [42]:\n",
    "kf\n",
    "Out[42]:\n",
    "array([0.72341463, 0.7404878 , 0.71268293, 0.71400683, 0.73108834])\n",
    "In [44]:\n",
    "Results=pd.DataFrame()\n",
    "Results['Model']=model_name\n",
    "Results['Technique']='TF-IDF'\n",
    "Results['Train Acc']=train_score\n",
    "Results['Cross Val - Min Accuarcy']=cv_min\n",
    "Results['Cross Val - Avg Accuarcy']=cv_av\n",
    "Results['Cross Val - Max Accuarcy']=cv_max\n",
    "Results['Cross Val - Std of Accuarcy']=cv_std\n",
    "Results['Test']=test_score\n",
    "Results\n",
    "Out[44]:\n",
    "Model\tTechnique\tTrain Acc\tCross Val - Min Accuarcy\tCross Val - Avg Accuarcy\tCross Val - Max Accuarcy\tCross Val - Std of Accuarcy\tTest\n",
    "0\tNaive Bayes\tTF-IDF\t0.720043\t0.660322\t0.665886\t0.667805\t0.002800\t0.698770\n",
    "1\tDecision Tree\tTF-IDF\t0.995219\t0.666179\t0.672228\t0.682439\t0.005666\t0.695128\n",
    "2\tRandom Forest\tTF-IDF\t0.994633\t0.741951\t0.755757\t0.762811\t0.008089\t0.772541\n",
    "3\tBagging\tTF-IDF\t0.969653\t0.712683\t0.724336\t0.740488\t0.010486\t0.742714\n",
    "In [45]:\n",
    "result = model_rf.predict(X_test)\n",
    "In [46]:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, result)\n",
    "\n",
    "df_cm = pd.DataFrame(conf_mat, index = [i for i in ['negative', 'neutral','positive']],\n",
    "                  columns = [i for i in ['negative', 'neutral','positive']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')\n",
    "Out[46]:\n",
    "<matplotlib.axes._subplots.AxesSubplot at 0x1c323eb7f88>\n",
    "\n",
    "Summary:\n",
    "\n",
    "We used dataset which has tweets in text format and their sentiment type (positive, negative and neutral).\n",
    "The goal was to build a model for text-classification.\n",
    "We Pre-processed the data using variuos techniques and libraries.\n",
    "The pre-precessed data is converted to numbers, so that we can feed the data in the model.\n",
    "After building the classification model, we predicted the result for the test data.\n",
    "After that we saw that using the above techniques, our model performed good in perspective of how the text classification models perform.\n",
    "One more way to increase accuracy is to use different variations of Pre-processing techniques.\n",
    "SMOTE \n",
    "In [47]:\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "In [48]:\n",
    "smt = SMOTE(random_state=0)\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)\n",
    "In [49]:\n",
    "X_train.shape\n",
    "Out[49]:\n",
    "(19092, 10000)\n",
    "In [50]:\n",
    "y_train.shape\n",
    "Out[50]:\n",
    "(19092,)\n",
    "In [51]:\n",
    "y_train.value_counts()\n",
    "Out[51]:\n",
    "neutral     6364\n",
    "positive    6364\n",
    "negative    6364\n",
    "Name: airline_sentiment, dtype: int64\n",
    "In [52]:\n",
    "model_name=[]\n",
    "train_score=[]\n",
    "test_score=[]\n",
    "cv_min=[]\n",
    "cv_max=[]\n",
    "cv_av=[]\n",
    "cv_std=[]\n",
    "std_score=[]\n",
    "\n",
    "i=0\n",
    "for m in models:\n",
    "    \n",
    "    m.fit(X_train, y_train)\n",
    "    \n",
    "    print(model_nm[i])\n",
    "    \n",
    "    model_name.append(model_nm[i])\n",
    "    i+=1\n",
    "    \n",
    "    train_score.append(m.score(X_train,y_train))\n",
    "    test_score.append(m.score(X_test,y_test))\n",
    "    \n",
    "    sf=StratifiedKFold(n_splits=5, random_state=20, shuffle=True)\n",
    "    kf=cross_val_score(m, X_train, y_train, cv=sf)\n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_min.append(np.min(kf))\n",
    "    cv_max.append(np.max(kf))\n",
    "    cv_av.append(np.mean(kf))\n",
    "    cv_std.append(np.std(kf))\n",
    "Naive Bayes\n",
    "Decision Tree\n",
    "Random Forest\n",
    "Bagging\n",
    "In [53]:\n",
    "Results_Smote=pd.DataFrame()\n",
    "Results_Smote['Model']=model_name\n",
    "Results_Smote['Train Acc']=train_score\n",
    "Results_Smote['Cross Val - Min Accuarcy']=cv_min\n",
    "Results_Smote['Cross Val - Avg Accuarcy']=cv_av\n",
    "Results_Smote['Cross Val - Max Accuarcy']=cv_max\n",
    "Results_Smote['Cross Val - Std of Accuarcy']=cv_std\n",
    "Results_Smote['Test']=test_score\n",
    "Results_Smote\n",
    "Out[53]:\n",
    "Model\tTrain Acc\tCross Val - Min Accuarcy\tCross Val - Avg Accuarcy\tCross Val - Max Accuarcy\tCross Val - Std of Accuarcy\tTest\n",
    "0\tNaive Bayes\t0.883826\t0.833159\t0.840404\t0.847342\t0.005786\t0.765483\n",
    "1\tDecision Tree\t0.995338\t0.759298\t0.766080\t0.768727\t0.003613\t0.688525\n",
    "2\tRandom Forest\t0.995286\t0.881090\t0.888382\t0.893428\t0.004999\t0.767987\n",
    "3\tBagging\t0.982401\t0.811420\t0.818982\t0.822420\t0.004040\t0.726548\n",
    "In [ ]:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
